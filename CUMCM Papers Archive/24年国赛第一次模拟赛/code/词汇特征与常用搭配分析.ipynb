{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\60938\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.448 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import jieba\n",
    "from collections import Counter\n",
    "\n",
    "scholar_data = pd.read_excel('学者撰写的论文摘要.xlsx')\n",
    "ai_data = pd.read_excel('AI生成的论文摘要.xlsx')\n",
    "\n",
    "scholar_abstracts = scholar_data['论文摘要']\n",
    "ai_abstracts = ai_data['论文摘要']\n",
    "\n",
    "# 加载停用词表\n",
    "def load_stopwords(filepath):\n",
    "    with open(filepath, 'r', encoding='utf-8') as file:\n",
    "        stopwords = set(file.read().strip().split('\\n'))\n",
    "    return stopwords\n",
    "\n",
    "stopwords = load_stopwords('stopwords.txt')\n",
    "\n",
    "# 分词并去除停用词\n",
    "def jieba_tokenizer(text):\n",
    "    words = jieba.cut(text)\n",
    "    return ' '.join([word for word in words if word not in stopwords])\n",
    "\n",
    "scholar_abstracts = scholar_abstracts.apply(jieba_tokenizer)\n",
    "ai_abstracts = ai_abstracts.apply(jieba_tokenizer)\n",
    "\n",
    "# 统计词频函数\n",
    "def count_words(texts):\n",
    "    all_words = []\n",
    "    for text in texts:\n",
    "        all_words.extend(text.split())\n",
    "    return Counter(all_words)\n",
    "\n",
    "scholar_words = count_words(scholar_abstracts)\n",
    "ai_words = count_words(ai_abstracts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "学者撰写的论文摘要:\n",
      "[(',', 747), ('学习', 218), ('方法', 170), ('深度', 159), ('进行', 149), ('算法', 139), ('目标', 137), ('研究', 137), ('检测', 134), ('中', 120), ('模型', 102), ('领域', 95), ('数据', 89), ('技术', 86), ('问题', 83)]\n",
      "AI生成的论文摘要:\n",
      "[('本文', 244), ('方法', 168), ('应用', 156), ('学习', 152), ('中', 150), ('深度', 115), ('研究', 107), ('领域', 88), ('算法', 84), ('模型', 83), ('检测', 82), ('技术', 78), ('（', 76), ('）', 76), ('方向', 73)]\n"
     ]
    }
   ],
   "source": [
    "# 输出词频最高的前15个词\n",
    "print('学者撰写的论文摘要:')\n",
    "print(scholar_words.most_common(15))\n",
    "print('AI生成的论文摘要:')\n",
    "print(ai_words.most_common(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "学者撰写的论文摘要独有词:\n",
      "[(';', 70), (')', 70), ('(', 67), ('代表性', 16), ('文中', 15), ('归纳', 15), ('目前', 13), ('分为', 12), ('角度', 11), ('值', 10), ('算子', 10), ('\"', 10), ('所提', 9), ('节点', 9), ('科学', 9)]\n",
      "AI生成的论文摘要独有词:\n",
      "[('揭示', 16), ('涵盖', 8), ('系列', 7), ('FCN', 7), ('最新进展', 7), ('表现出色', 6), ('交通系统', 6), ('系统阐述', 5), ('如多', 4), ('层次化', 4), ('DQN', 4), ('高精度', 4), ('局限', 4), ('优异', 4), ('现代', 4)]\n"
     ]
    }
   ],
   "source": [
    "# 统计独有词\n",
    "scholar_unique_words = set(scholar_words.keys()) - set(ai_words.keys())\n",
    "ai_unique_words = set(ai_words.keys()) - set(scholar_words.keys())\n",
    "\n",
    "# 分别输出独有词频数最高的前15个词\n",
    "print('学者撰写的论文摘要独有词:')\n",
    "print(Counter({word: scholar_words[word] for word in scholar_unique_words}).most_common(15))\n",
    "print('AI生成的论文摘要独有词:')\n",
    "print(Counter({word: ai_words[word] for word in ai_unique_words}).most_common(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import ngrams\n",
    "\n",
    "# 定义函数来生成 N-gram，并计算频率\n",
    "def generate_ngrams(tokens, n):\n",
    "    n_grams = ngrams(tokens, n)\n",
    "    return [' '.join(gram) for gram in n_grams]\n",
    "\n",
    "# 定义函数来统计 N-gram 的频率并返回前 top_n 个\n",
    "def top_ngrams_by_frequency(abstracts, min_n, max_n, top_n):\n",
    "    all_ngrams = Counter()\n",
    "    for abstract in abstracts:\n",
    "        words = abstract.split()\n",
    "        for n in range(min_n, max_n + 1):\n",
    "            ngrams_list = generate_ngrams(words, n)  # 生成 N-gram\n",
    "            all_ngrams.update(ngrams_list)\n",
    "    top_ngrams = all_ngrams.most_common(top_n)\n",
    "    return top_ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义输出 top_n 个高频搭配的函数\n",
    "def print_top_ngrams(top_ngrams, category):\n",
    "    print(f\"Top {len(top_ngrams)} {category} 搭配：\")\n",
    "    for ngram, freq in top_ngrams:\n",
    "        print(f\"{ngram}: {freq}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 30 学者撰写 搭配：\n",
      "深度 学习: 106\n",
      "目标 检测: 92\n",
      "强化 学习: 50\n",
      "计算机 视觉: 38\n",
      "问题 ,: 33\n",
      "检测 算法: 28\n",
      "目标 检测 算法: 27\n",
      "方法 ,: 27\n",
      "卷积 神经网络: 26\n",
      "智能 体: 26\n",
      ", 提出: 25\n",
      "机器 学习: 24\n",
      ", 深度: 22\n",
      "; 最后: 20\n",
      "提出 一种: 20\n",
      "路径 规划: 19\n",
      "检测 方法: 19\n",
      "研究 方向: 18\n",
      ", 介绍: 18\n",
      "进行 分析: 18\n",
      "发展 方向: 17\n",
      "数据 集: 17\n",
      "最后 ,: 17\n",
      "进行 展望: 16\n",
      "进行 总结: 16\n",
      "语义 分割: 16\n",
      "实验 结果表明: 16\n",
      ", 提出 一种: 16\n",
      ", 深度 学习: 15\n",
      ", 分析: 15\n"
     ]
    }
   ],
   "source": [
    "# 统计学者撰写的摘要中的 N-gram 频率并输出前15个\n",
    "scholar_top_ngrams = top_ngrams_by_frequency(scholar_abstracts, 2, 10, 30)\n",
    "print_top_ngrams(scholar_top_ngrams, \"学者撰写\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 30 AI生成 搭配：\n",
      "深度 学习: 91\n",
      "目标 检测: 55\n",
      "研究 方向: 42\n",
      "本文 综述: 41\n",
      "实际 应用: 35\n",
      "应用 中: 35\n",
      "中 应用: 35\n",
      "强化 学习: 33\n",
      "未来 研究: 30\n",
      "最后 本文: 30\n",
      "未来 研究 方向: 29\n",
      "实际 应用 中: 29\n",
      "卷积 神经网络: 28\n",
      "本文 提出: 28\n",
      "计算机 视觉: 26\n",
      "神经网络 （: 26\n",
      "未来 发展: 25\n",
      "发展 方向: 24\n",
      "提出 一种: 24\n",
      "本文 提出 一种: 24\n",
      "本文 系统: 23\n",
      "未来 发展 方向: 23\n",
      "本文 详细: 21\n",
      "本文 探讨: 20\n",
      "本文 总结: 20\n",
      "检测 算法: 19\n",
      "目标 检测 算法: 18\n",
      "卷积 神经网络 （: 18\n",
      "详细 讨论: 18\n",
      "实验 结果表明: 18\n"
     ]
    }
   ],
   "source": [
    "# 统计AI生成的摘要中的 N-gram 频率并输出前15个\n",
    "ai_top_ngrams = top_ngrams_by_frequency(ai_abstracts, 2, 10, 30)\n",
    "print_top_ngrams(ai_top_ngrams, \"AI生成\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
